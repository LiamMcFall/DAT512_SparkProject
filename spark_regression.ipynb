{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "## Liam McFall\n",
    "\n",
    "1) Strategy and Overview\n",
    "\n",
    "For this project, my strategy is to try different types of models beyond basic least squares regression. I am intrigued by the potential for tree based models. I expect a random forest to perform better than the basic regression, which is a low bar to clear since the basic regression I created was essentially meaningless. I also plan on updating the data set using data from the same source that I originally used, this way I have as much data as possible at my disposal. Something that we don't have access to as much with a regular linear model that we do have when using something tree based, is a graph that shows feature importance. I think that this will be an interesting factor to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Refining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://liams-mbp-2:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11694df10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Project_2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "display(spark)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- X: integer (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long_: double (nullable = true)\n",
      " |-- Confirmed: integer (nullable = true)\n",
      " |-- Deaths: integer (nullable = true)\n",
      " |-- Active: integer (nullable = true)\n",
      " |-- Combined_Key: string (nullable = true)\n",
      " |-- State_popDensity: double (nullable = true)\n",
      "\n",
      "+---+---------+--------------+-----------+------------+---------+------+------+--------------------+----------------+\n",
      "|  X|   County|         State|        Lat|       Long_|Confirmed|Deaths|Active|        Combined_Key|State_popDensity|\n",
      "+---+---------+--------------+-----------+------------+---------+------+------+--------------------+----------------+\n",
      "|  1|Abbeville|South Carolina|34.22333378|-82.46170658|       33|     0|    33|Abbeville, South ...|        173.3174|\n",
      "|  2|   Acadia|     Louisiana| 30.2950649|-92.41419698|      140|    10|   130|Acadia, Louisiana...|        107.5175|\n",
      "|  3| Accomack|      Virginia|37.76707161|-75.63234615|      429|     7|   422|Accomack, Virgini...|        218.4403|\n",
      "|  4|      Ada|         Idaho| 43.4526575|-116.2415516|      717|    19|   698|      Ada, Idaho, US|         22.0969|\n",
      "|  5|    Adair|          Iowa|41.33075609|-94.47105874|        3|     0|     3|     Adair, Iowa, US|         56.9284|\n",
      "|  6|    Adair|      Kentucky|37.10459774|-85.28129668|       82|    13|    69| Adair, Kentucky, US|        113.9566|\n",
      "|  7|    Adair|      Missouri|40.19058551|-92.60078167|       12|     0|    12| Adair, Missouri, US|         89.7453|\n",
      "|  8|    Adair|      Oklahoma|35.88494195|-94.65859267|       68|     3|    65| Adair, Oklahoma, US|         57.6547|\n",
      "|  9|    Adams|      Colorado|39.87432092|-104.3362578|     1952|    75|  1877| Adams, Colorado, US|         56.4011|\n",
      "| 10|    Adams|         Idaho|44.89333571|-116.4545247|        3|     0|     3|    Adams, Idaho, US|         22.0969|\n",
      "+---+---------+--------------+-----------+------------+---------+------+------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2885"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loading\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "data_schema = [#StructField('_c0', IntegerType(), True),\n",
    "               StructField('X', IntegerType(), True), \n",
    "               StructField('County', StringType(), True), \n",
    "               StructField('State', StringType(), True), \n",
    "               StructField('Lat', DoubleType(), True), \n",
    "               StructField('Long_', DoubleType(), True), \n",
    "               StructField('Confirmed', IntegerType(), True), \n",
    "               StructField('Deaths', IntegerType(), True), \n",
    "               StructField('Active', IntegerType(), True), \n",
    "               StructField('Combined_Key', StringType(), True), \n",
    "               StructField('State_popDensity', DoubleType(), True)]\n",
    "\n",
    "final_struc = StructType(fields=data_schema)\n",
    "\n",
    "covid = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .schema(final_struc)\\\n",
    "    .load(\"/Users/liammcfall/Desktop/usCovid_byCounty.csv\")\n",
    "\n",
    "covid.printSchema()\n",
    "covid.show(10)\n",
    "\n",
    "covid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Vector Sample\n",
      "+--------------------+---------+\n",
      "|            features|Confirmed|\n",
      "+--------------------+---------+\n",
      "|[34.22333378,-82....|       33|\n",
      "|[30.2950649,-92.4...|      140|\n",
      "|[37.76707161,-75....|      429|\n",
      "|[43.4526575,-116....|      717|\n",
      "|[41.33075609,-94....|        3|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Training Vector Sample\n",
      "+--------------------+---------+\n",
      "|            features|Confirmed|\n",
      "+--------------------+---------+\n",
      "|[20.86399628,-156...|      116|\n",
      "|[21.45803166,-157...|      405|\n",
      "|[22.03935037,-159...|       21|\n",
      "|[25.20904673,-81....|       80|\n",
      "|[25.6112362,-80.5...|    13371|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Vector Sample\n",
      "+--------------------+---------+\n",
      "|            features|Confirmed|\n",
      "+--------------------+---------+\n",
      "|[19.60121157,-155...|       74|\n",
      "|[26.15184651,-80....|     5553|\n",
      "|[26.39418217,-98....|      353|\n",
      "|[26.64676272,-80....|     3480|\n",
      "|[26.90131002,-81....|      294|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Set up\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['Lat', 'Long_', 'State_popDensity'], \n",
    "                                  outputCol = 'features', \n",
    "                                  handleInvalid = \"skip\")\n",
    "vecCovid = vectorAssembler.transform(covid)\n",
    "#vecCovid.count()\n",
    "\n",
    "vecCovid = vecCovid.select(['features', 'Confirmed'])\n",
    "print(\"Full Vector Sample\")\n",
    "vecCovid.show(5)\n",
    "\n",
    "train, test = vecCovid.randomSplit([.75,.25], 7)\n",
    "print(\"Training Vector Sample\")\n",
    "train.show(5)\n",
    "print(\"Test Vector Sample\")\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Training Data\n",
      "Coefficients: [48.027931434034855,18.018296559026833,1.6701261863379708]\n",
      "Intercept: 0.0\n",
      "MSE: 18908488.878441\n",
      "r2: 0.030726\n",
      "Evaluation of Model using Test Data\n",
      "MAE: 539.824126\n",
      "MSE: 1036130.131559\n",
      "r2: 0.061021\n"
     ]
    }
   ],
   "source": [
    "print(\"Rerunning project 2 Linear regression with the new data\")\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='Confirmed',fitIntercept=False)\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "print(\"Evaluation of Training Data\")\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "print(\"MSE: %f\" % lr_model.summary.meanSquaredError)\n",
    "print(\"r2: %f\" % lr_model.summary.r2)\n",
    "\n",
    "print(\"Evaluation of Model using Test Data\")\n",
    "lr_evaluation_summary = lr_model.evaluate(test)\n",
    "print(\"MAE: %f\" % lr_evaluation_summary.meanAbsoluteError)\n",
    "print(\"MSE: %f\" % lr_evaluation_summary.meanSquaredError)\n",
    "print(\"r2: %f\" % lr_evaluation_summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Evaluation of Model using Test Data\n",
      "MAE: 336.517116\n",
      "MSE: 596402.434308\n",
      "r2: 0.422709\n",
      "Feature Importance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 0.442, 1: 0.288, 2: 0.27})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol = 'features', labelCol = 'Confirmed', seed = 3)\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "rf_model.featureImportances\n",
    "predictions = rf_model.transform(test)\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"Confirmed\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "evaluator_mse = RegressionEvaluator(\n",
    "    labelCol=\"Confirmed\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator_mse.evaluate(predictions)\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"Confirmed\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Evaluation of Model using Test Data\")\n",
    "print(\"MAE: %f\" % mae)\n",
    "print(\"MSE: %f\" % mse)\n",
    "print(\"r2: %f\" % r2)\n",
    "\n",
    "print(\"Feature Importance\")\n",
    "rf_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF performance vs Linear Model\n",
      "MAE: 0.623383\n",
      "MSE: 0.575606\n",
      "r2: 6.927283\n"
     ]
    }
   ],
   "source": [
    "rf_lm_mae = mae/lr_evaluation_summary.meanAbsoluteError\n",
    "rf_lm_mse = mse/lr_evaluation_summary.meanSquaredError\n",
    "rf_lm_r2 = r2/lr_evaluation_summary.r2\n",
    "\n",
    "print(\"RF performance vs Linear Model\")\n",
    "print(\"MAE: %f\" % rf_lm_mae)\n",
    "print(\"MSE: %f\" % rf_lm_mse)\n",
    "print(\"r2: %f\" % rf_lm_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Verify the Model Metrics\n",
    "\n",
    "After running the model using the same test and training data in both a linear regression, and a Random Forest, it is clear that the random forest is the superior performer. The Random Forest had a 38% smaller mean absolute error, and a 43% smaller mean squared error. Additionally, the random forest had an R^2 6.9 times the linear regression model, explaining more than 42% of the variance in the model. Using the random forest, we are also able to look feature importance. Latitude is shown as the most importance feature in this model with 44% of the importance. Surprisingly, population density is the least important factor of the 3 at 27%, but I think this is probably due to the lack of variance in this feature since we are using state population density, not at the county level due. I am really happy with how much better this model fits the data compared to the basic linear regression model using the same data. I think that makes sense due to the fact that this data has a time series element and all of these counties are going to be at different locations on their own curves. I wasn't quite sure how to account for this, but the random forest ended up performing reasonably well within that constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Conclusions\n",
    "\n",
    "    - A regression is using certain pieces of an observation (ie. independent variables) in order to predict a dependent variable. A regression is when this prediction is for a continuous value, where as a discrete prediction would be a classification. In this case, a Random forest regression was used. What happens in this is there are a number of individual decision trees created with the algorithm, each created using a subset of observations of the training data with resampling of the data. This means that in the \"training\" set, all observations may not be used but some may be used more than once. The prediction for each observation is then the average of the predictions from all of the individual decision trees.\n",
    "    - An excel pivot table is just going to be aggregations of data, not predicting anything. The purpose of my model would be to look at what counties are doing well or poorly in terms of how many cases they SHOULD have. The prediction represents the number that they should have given the data, and the difference between the prediction and the confirmed case numbers is what we are really looking for. Using an excel pivot ta le you would not be able to do this comparison. While a linear model would be quite meaningless in this excersize as shown by only 6% of the variance being explained by the model, the Random forest has an R^2 of above .42 which means that this extremely basic model with only 3 variables is able to explain more than 42% of the data which is a great starting point.\n",
    "    - My use case for this analysis did not really change, the model itself just got better when using a random forest compared to a linear regression. The key predictor in the model was latitude, but longitude and state population density were also important variables. I am confident in population density being an important factor in transmission, but I would have preferred to have been able to find it at the county level instead of using state population density as a proxy. The states that have large urban centers, but also have large rural areas are going to be thrown off by this lack of specificity. I'm on the fence about how important both latitude and longitude are. These are more descriptive factors than anything. I suppose if climate does play a factor in transmission then latitude may in fact be important, but epidemiological research that has come out surrounding COVID has been inconclusive on that aspect. I think a great feature to add would be \"days since 1st infection.\" Adding this feature would incorporate some of the time effects on the model that it currently lacks. This would add a way to incorporate where along each county's individual curve they are. In terms of boundary condition, the model must be bound to have an intercept of greater than 0, because \"Confirmed\" must be positive. It is impossible to have \"Confirmed\" cases be less than 0, so the intercept can't be less than 0 either. Overall, this isn't a completely linear data set which is probably why a random forest works so much better than a standard linear regression. There is more flexibility afforded by using a random forest as the model, and than is showcased by how much better it performed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
